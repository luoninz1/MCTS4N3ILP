#!/bin/bash

#SBATCH --job-name=placeholder     ## job name
#SBATCH -A NCKAPLAN_LAB     ## Class account to charge resources
#SBATCH -p standard                ## partition name
#SBATCH --nodes=1             ## (-N) number of nodes the job will use
#SBATCH --ntasks=1            ## (-n) number of processes to be launched
#SBATCH -c 1                 ## number of cores (increased for better parallelism)
#SBATCH --mem=6G           ## placeholder memory for dynamic allocation
#SBATCH --time 14-00:00:00     ## time limit (3 days)
#SBATCH --error=logs/slurm-%J.err  ## error log file
#SBATCH --output=logs/slurm-%J.out ## output log file
#SBATCH --mail-type=fail,end
#SBATCH --mail-user=luoninz1@uci.edu

module load miniconda3/24.9.2
source /opt/apps/miniconda3/24.9.2/etc/profile.d/conda.sh
conda activate mcgs-env

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export MKL_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OPENBLAS_NUM_THREADS=$SLURM_CPUS_PER_TASK
export NUMBA_NUM_THREADS=$SLURM_CPUS_PER_TASK
export NUMBA_THREADING_LAYER=omp

# ===== Expected resources (derived from start) vs Actual resources (after sbatch override) =====
# Rule: starting from start=53, every +20 â‡’ +6G memory & +1 CPU core; initial is 1 core / 6G
start="$1"
inc=$(( (start-53)/20 ))
(( inc < 0 )) && inc=0

exp_cpus=$(( 1 + inc ))
exp_mem_g=$(( 6 + 6*inc ))  # GiB

# Dynamically update job name, include start for identification
scontrol update JobId=$SLURM_JOB_ID JobName="n${start}_mcts_largest_complete_test" >/dev/null 2>&1 || true

echo "========== RESOURCE CHECK (JobID=$SLURM_JOB_ID) =========="
echo "Start parameter        : ${start}"
echo "Expected (rule)        : CPUs=${exp_cpus}, MEM=${exp_mem_g}G"
echo "SLURM_CPUS_PER_TASK    : ${SLURM_CPUS_PER_TASK:-N/A}"
echo "SLURM_JOB_CPUS_PER_NODE: ${SLURM_JOB_CPUS_PER_NODE:-N/A}"
echo "SLURM_MEM_PER_CPU      : ${SLURM_MEM_PER_CPU:-N/A}  (MB, if defined by site)"
echo "SLURM_MEM_PER_NODE     : ${SLURM_MEM_PER_NODE:-N/A} (MB, if defined by site)"
echo "--- scontrol show job (key fields) ---"
scontrol show job $SLURM_JOB_ID | awk '
/JobName=|Partition=|Ntasks=|NumCPUs=|TRES=|MinMemory/{
  gsub(/\r/,""); print
}'
echo "=========================================================="

# $1 = start value, $2 = step size
python largest_complete_v2.py \
    --start "$1" \
    --end 104 \
    --step 104 \
    --repeat 1 \
    > logs/mcts_largest_complete_v2_n_"$1".log